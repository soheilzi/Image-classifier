{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one is done in a different notebook called phase1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from copy import deepcopy\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \n",
    "    def __init__(self, data, labels, n_classes, batch_size=None, shuffle=False):\n",
    "\n",
    "        assert len(data)==len(labels)\n",
    "        self.__n_classes = n_classes\n",
    "        self.__batch_size = batch_size\n",
    "        self.__shuffle = shuffle\n",
    "        self.__data = data\n",
    "        self.__onehot_labels = self.__onehot(labels, self.__n_classes)\n",
    "    \n",
    "    def __onehot(self, labels, n_classes):\n",
    "        onehot_vectors = np.zeros((labels.size, n_classes))\n",
    "        onehot_vectors[np.arange(labels.size),labels] = 1\n",
    "        return onehot_vectors\n",
    "    \n",
    "    def __shuffle_dataset(self):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(self.__data)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(self.__onehot_labels)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.__shuffle:\n",
    "            self.__shuffle_dataset()\n",
    "            \n",
    "        if self.__batch_size==None:\n",
    "            yield (np.matrix(self.__data), np.matrix(self.__onehot_labels))\n",
    "            return\n",
    "            \n",
    "        for idx in range(0, len(self.__data), self.__batch_size):\n",
    "            yield (np.matrix(self.__data[idx:idx+self.__batch_size]), \n",
    "                   np.matrix(self.__onehot_labels[idx:idx+self.__batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identical:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def __val(self, matrix):\n",
    "        identical_value = np.matrix(matrix, dtype=float)\n",
    "        return identical_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        identical_derivative = np.matrix(np.full(np.shape(temp), 1.))\n",
    "        return identical_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__val(matrix)\n",
    "    \n",
    "\n",
    "class Relu:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def __val(self, matrix):\n",
    "        relu_value = deepcopy(matrix)\n",
    "        relu_value[relu_value <= 0] = 0\n",
    "        return relu_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        relu_derivative = deepcopy(matrix)\n",
    "        relu_derivative[relu_derivative > 0] = 1\n",
    "        relu_derivative[relu_derivative <= 0] = 0\n",
    "        return relu_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__val(matrix)\n",
    "\n",
    "    \n",
    "class LeakyRelu:\n",
    "    \n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        self.negative_slope = 0.01\n",
    "    \n",
    "    def __val(self, matrix):\n",
    "        leacky_relu_value = np.matrix(np.maximum(matrix, matrix * self.negative_slope), dtype=float)\n",
    "        return leacky_relu_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        leacky_relu_derivative = np.matrix(matrix, dtype=float)\n",
    "        leacky_relu_derivative[leacky_relu_derivative >= 0] = 1\n",
    "        leacky_relu_derivative[leacky_relu_derivative < 0] = -self.negative_slope\n",
    "        return leacky_relu_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__val(matrix)\n",
    "\n",
    "    \n",
    "class Sigmoid:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def __val(self, matrix):\n",
    "        sigmoid_value = np.matrix(1/(1 + np.exp(-matrix)), dtype=float)\n",
    "        return sigmoid_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        sigmoid_value = self.__val(matrix)\n",
    "        sigmoid_derivative = np.matrix(np.multiply(sigmoid_value,(1 - sigmoid_value)), dtype=float)\n",
    "        return sigmoid_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__val(matrix)\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def __val(self, matrix):\n",
    "        matrix = matrix - np.max(matrix, axis=1)\n",
    "        softmax_value = np.matrix(np.exp(matrix) / np.sum(np.exp(matrix), axis=1), dtype=float)\n",
    "        return softmax_value\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__val(matrix)\n",
    "    \n",
    "class Tanh:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def __val(self, matrix):\n",
    "        positive_exponent = np.exp(matrix)\n",
    "        negative_exponent = np.exp(-matrix)\n",
    "        tanh_value = np.matrix((positive_exponent - negative_exponent) / (positive_exponent + negative_exponent), dtype=float)\n",
    "        return tanh_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        tanh_derivative = np.matrix(1 - np.power(self.__val(matrix), 2), dtype=float)\n",
    "        return tanh_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__val(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy: #(with softmax)\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def __val(self, true_val, expected_val):\n",
    "        assert np.shape(true_val)==np.shape(expected_val)\n",
    "        softmax = Softmax()\n",
    "        cross_entropy_value = np.multiply(-expected_val, np.log(np.clip(softmax(true_val), 1e-50, 1 - 1e-50))).sum(axis = 1) \n",
    "        return cross_entropy_value\n",
    "        \n",
    "    def derivative(self, true_val, expected_val):\n",
    "        assert np.shape(true_val)==np.shape(expected_val)\n",
    "        softmax = Softmax()\n",
    "        return softmax(true_val + 1e-50) - expected_val\n",
    "    \n",
    "    def __call__(self, true_val, expected_val):\n",
    "        return self.__val(true_val, expected_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    DEFAULT_LOW, DEFAULT_HIGH, DEFAULT_MEAN, DEFAULT_VAR, CLIP = 0, 0.05, 0., 1., 10\n",
    "  \n",
    "    def __init__(self, input_size, output_size, \n",
    "                 activation=Identical(), initial_weight='uniform', **initializing_parameters):\n",
    "        \n",
    "        assert type(initial_weight)==str, 'Undefined activation function!'\n",
    "        \n",
    "        self.__weight_initializer_dict = {'uniform':self.__uniform_weight, 'normal':self.__normal_weight}\n",
    "        \n",
    "        assert initial_weight in self.__weight_initializer_dict, 'Undefined weight initialization function!'\n",
    "\n",
    "\n",
    "        self.__n_neurons = output_size\n",
    "        weight_initializer = self.__weight_initializer_dict[initial_weight]\n",
    "        self.__weight = weight_initializer(input_size, self.__n_neurons, **initializing_parameters)\n",
    "        self.__bias = weight_initializer(1, self.__n_neurons, **initializing_parameters)\n",
    "        self.__activation = activation\n",
    "        \n",
    "        self.__last_input = None\n",
    "        self.__last_activation_input = None\n",
    "        self.__last_activation_output = None\n",
    "        self.__last_activation_derivative = None\n",
    "        \n",
    "    def forward(self, layer_input):\n",
    "        assert np.ndim(layer_input)==2\n",
    "        assert np.size(self.__weight,0) == np.size(layer_input,1)\n",
    "        assert np.size(self.__weight,1) == np.size(self.__bias)\n",
    "        self.__last_input = layer_input\n",
    "        self.__last_activation_input = layer_input.dot(self.__weight) + self.__bias\n",
    "        self.__last_activation_output = self.__activation(self.__last_activation_input)\n",
    "        self.__last_activation_derivative = np.squeeze(self.__activation.derivative(self.__last_activation_input))\n",
    "        \n",
    "        return self.__last_activation_output\n",
    "    \n",
    "    def update_weights(self, backprop_tensor, lr):\n",
    "        assert np.ndim(backprop_tensor)==2\n",
    "        assert np.size(backprop_tensor,0) == np.size(self.__last_activation_derivative,0)\n",
    "        assert np.size(backprop_tensor,1) == self.__n_neurons\n",
    "        \n",
    "        backprop_tensor = np.squeeze(backprop_tensor)\n",
    "\n",
    "        b2 = np.multiply(backprop_tensor, self.__last_activation_derivative)\n",
    "\n",
    "        dw = self.__last_input.T.dot(b2)\n",
    "\n",
    "        db = np.sum(b2, axis=0)\n",
    "\n",
    "        new_backprop_tensor = b2.dot(self.__weight.T)\n",
    "        self.__bias = self.__bias - (lr * db)\n",
    "        self.__weight = self.__weight - (lr * dw)\n",
    "        \n",
    "        \n",
    "        return new_backprop_tensor\n",
    "\n",
    "    def __uniform_weight(self, dim1, dim2, **initializing_parameters):\n",
    "        low, high = self.DEFAULT_LOW, self.DEFAULT_HIGH\n",
    "        if 'low' in initializing_parameters.keys(): low = initializing_parameters['low']\n",
    "        if 'high' in initializing_parameters.keys(): high = initializing_parameters['high']\n",
    "        weights = np.random.uniform(low, high, dim1 * dim2)\n",
    "        weights = weights.reshape(dim1, dim2)\n",
    "        return weights\n",
    "\n",
    "    def __normal_weight(self, dim1, dim2, **initializing_parameters):\n",
    "        mean, var = self.DEFAULT_MEAN, self.DEFAULT_VAR\n",
    "        if 'mean' in initializing_parameters.keys(): mean = initializing_parameters['mean']\n",
    "        if 'var' in initializing_parameters.keys(): var = initializing_parameters['var']\n",
    "        weights = np.random.normal(mean, var, dim1 * dim2)\n",
    "        weights = weights.reshape(dim1, dim2)\n",
    "        return weights\n",
    "    \n",
    "    @property\n",
    "    def n_neurons(self): return self.__n_neurons\n",
    "    \n",
    "    @property\n",
    "    def weight(self): return self.__weight\n",
    "    \n",
    "    @property\n",
    "    def bias(self): return self.__bias\n",
    "    \n",
    "    @property\n",
    "    def activation(self): return self.__activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        \n",
    "        self.__input_shape = input_shape\n",
    "        self.__output_shape = None\n",
    "        \n",
    "        self.__layers_list = []\n",
    "        \n",
    "        self.__lr = None\n",
    "        self.__loss = None\n",
    "\n",
    "        \n",
    "    def add_layer(self, n_neurons, activation=Sigmoid(), initial_weight='uniform', **initializing_parameters):\n",
    "         \n",
    "        assert type(n_neurons)==int, \"Invalid number of neurons for the layer!\"\n",
    "        assert n_neurons>0, \"Invalid number of neurons for the layer!\"\n",
    "        \n",
    "        n_prev_neurons = self.__input_shape if len(self.__layers_list)==0 else self.__layers_list[-1].n_neurons\n",
    "        new_layer = Layer(n_prev_neurons, n_neurons, activation, initial_weight, **initializing_parameters)\n",
    "        self.__layers_list.append(new_layer)\n",
    "        self.__output_shape = self.__layers_list[-1].n_neurons \n",
    "      \n",
    "    \n",
    "    def set_training_param(self, loss=CrossEntropy(), lr=1e-3):\n",
    "        assert self.__layers_list, \"Uncomplete model!\"\n",
    "        self.__loss = loss\n",
    "        self.__lr = lr\n",
    "    \n",
    "    \n",
    "    def forward(self, network_input):\n",
    "        assert type(self.__output_shape) != None, \"Model is not compiled!\"\n",
    "        softmax = Softmax()\n",
    "        network_output = deepcopy(network_input)\n",
    "        for i in range(len(self.__layers_list)):\n",
    "            network_output = self.__layers_list[i].forward(network_output)\n",
    "            network_output = np.squeeze(network_output)\n",
    "        network_output = softmax(network_output)\n",
    "        return network_output\n",
    "    \n",
    "    \n",
    "    def fit(self, epochs, trainloader, testloader=None, print_results=True):\n",
    "        \n",
    "        assert type(self.__output_shape) != None, \"Model is not compiled!\"\n",
    "        assert type(self.__lr) != None and type(self.__loss) != None, \"Training paramenters are not set!\"\n",
    "\n",
    "        log = {\"train_accuracy\":[], \"train_loss\":[], \"test_accuracy\":[], \"test_loss\":[]}\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            if print_results: \n",
    "                print('Epoch {}:'.format(epoch)) \n",
    "                \n",
    "            average_accuracy, average_loss = self.__train(trainloader)\n",
    "            log['train_accuracy'].append(average_accuracy)\n",
    "            log['train_loss'].append(average_loss)\n",
    "            if print_results:\n",
    "                print('\\tTrain: Average Accuracy: {}\\tAverage Loss: {}'.format(average_accuracy, average_loss))\n",
    "            \n",
    "            if type(testloader) != type(None):\n",
    "                average_accuracy, average_loss = self.__test(testloader)\n",
    "                log['test_accuracy'].append(average_accuracy)\n",
    "                log['test_loss'].append(average_loss)\n",
    "                if print_results:\n",
    "                    print('\\tTest: Average Accuracy: {}\\tAverage Loss: {}'.format(average_accuracy, average_loss))\n",
    "                    \n",
    "        return log\n",
    "    \n",
    "    \n",
    "    def __train(self, trainloader):\n",
    "        bach_accuracies, batch_losses = [], []\n",
    "        for x_train, y_train in trainloader:\n",
    "            batch_accuracy, batch_loss = self.__train_on_batch(x_train, y_train)\n",
    "            bach_accuracies.append(batch_accuracy)\n",
    "            batch_losses.append(batch_loss)\n",
    "        return np.mean(bach_accuracies), np.mean(batch_losses)\n",
    "    \n",
    "    \n",
    "    def __test(self, testloader):\n",
    "        bach_accuracies, batch_losses = [], []\n",
    "        for x_test, y_test in testloader:\n",
    "            batch_accuracy, batch_loss = self.__test_on_batch(x_test, y_test)\n",
    "            bach_accuracies.append(batch_accuracy)\n",
    "            batch_losses.append(batch_loss)\n",
    "        return np.mean(bach_accuracies), np.mean(batch_losses)\n",
    "\n",
    "    \n",
    "    def __train_on_batch(self, x_batch, y_batch):\n",
    "        result = deepcopy(x_batch)\n",
    "        softmax = Softmax()\n",
    "        for i in range(len(self.__layers_list)):\n",
    "            result = self.__layers_list[i].forward(result)\n",
    "        loss = self.__loss(result, y_batch)\n",
    "        self.__update_weights(result, y_batch)\n",
    "        result = softmax(result)\n",
    "        \n",
    "        batch_accuracy = self.__compute_accuracy(result, y_batch)\n",
    "        batch_average_loss = np.mean(loss)\n",
    "    \n",
    "        return (batch_accuracy, batch_average_loss)\n",
    "        \n",
    "        \n",
    "    def __test_on_batch(self, x_batch, y_batch):\n",
    "        result = deepcopy(x_batch)\n",
    "        softmax = Softmax()\n",
    "        for i in range(len(self.__layers_list)):\n",
    "            result = self.__layers_list[i].forward(result)\n",
    "        loss = self.__loss(result, y_batch)\n",
    "        result = softmax(result)\n",
    "        \n",
    "        batch_accuracy = self.__compute_accuracy(result, y_batch)\n",
    "        batch_average_loss = np.mean(loss)\n",
    "        return (batch_accuracy, batch_average_loss)\n",
    "            \n",
    "        \n",
    "    def __get_labels(self, outputs):\n",
    "        labels = np.argmax(outputs, axis=1)\n",
    "        return labels\n",
    "    \n",
    "    \n",
    "    def __compute_accuracy(self, output, expected_output):\n",
    "        labels_output = self.__get_labels(output)\n",
    "        labels_expected = self.__get_labels(expected_output)\n",
    "        count_uncorrect = np.count_nonzero(labels_expected - labels_output)\n",
    "        accuracy = (len(labels_expected) - count_uncorrect) / len(labels_expected)\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def __update_weights(self, output, y_train):\n",
    "        back_prop_tensor = self.__loss.derivative(output, y_train)\n",
    "        for layer in self.__layers_list[::-1]:\n",
    "            back_prop_tensor = layer.update_weights(back_prop_tensor, self.__lr)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./Dataset/dataset/train_images.csv\")\n",
    "X_test = pd.read_csv(\"./Dataset/dataset/test_images.csv\")\n",
    "y_train = pd.read_csv(\"./Dataset/dataset/train_labels.csv\")\n",
    "y_test = pd.read_csv(\"./Dataset/dataset/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 785), (60000, 2), (15000, 785), (15000, 2))\n"
     ]
    }
   ],
   "source": [
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 784), (60000, 1), (15000, 784), (15000, 1))\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.drop(columns=['Unnamed: 0'], axis=1)\n",
    "y_test = y_test.drop(columns=['Unnamed: 0'], axis=1)\n",
    "X_train = X_train.drop(columns=['Unnamed: 0'], axis=1)\n",
    "X_test = X_test.drop(columns=['Unnamed: 0'], axis=1)\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "temp = X_test.to_numpy()\n",
    "X_test = temp\n",
    "temp = X_train.to_numpy()\n",
    "X_train = temp\n",
    "\n",
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "print(type(X_train), type(X_test), type(y_train), type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000,) (60000,)\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.reshape(len(y_test))\n",
    "y_train = y_train.reshape(len(y_train))\n",
    "\n",
    "print(y_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = Dataloader(X_train, y_train, n_classes=20, batch_size=64)\n",
    "testloader = Dataloader(X_test, y_test, n_classes=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = X_train.shape[1]\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.12936433901918976\tAverage Loss: 2.6833568682570212\n",
      "\tTest: Average Accuracy: 0.178125\tAverage Loss: 2.496684346265347\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.3386693763326226\tAverage Loss: 2.029533977150357\n",
      "\tTest: Average Accuracy: 0.43107269503546103\tAverage Loss: 1.869994682048095\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.5991138059701493\tAverage Loss: 1.3465136569411151\n",
      "\tTest: Average Accuracy: 0.5529033687943263\tAverage Loss: 1.5409856162658344\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.695262526652452\tAverage Loss: 1.0553869456164797\n",
      "\tTest: Average Accuracy: 0.6360372340425532\tAverage Loss: 1.3032039797026085\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.7641424573560768\tAverage Loss: 0.8403837845073577\n",
      "\tTest: Average Accuracy: 0.6847960992907801\tAverage Loss: 1.1317762859632907\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.8094682835820896\tAverage Loss: 0.6847780944550427\n",
      "\tTest: Average Accuracy: 0.720921985815603\tAverage Loss: 1.006930493193504\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.8372867803837953\tAverage Loss: 0.5821640137156665\n",
      "\tTest: Average Accuracy: 0.7448581560283689\tAverage Loss: 0.9263031623725919\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.8570928837953091\tAverage Loss: 0.5098785285289668\n",
      "\tTest: Average Accuracy: 0.7622783687943263\tAverage Loss: 0.8690049313969332\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.8723347547974414\tAverage Loss: 0.4548140408364511\n",
      "\tTest: Average Accuracy: 0.7752437943262412\tAverage Loss: 0.831148955129541\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.8830457089552238\tAverage Loss: 0.4142360726188368\n",
      "\tTest: Average Accuracy: 0.7832668439716312\tAverage Loss: 0.7977800480410477\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.8911747068230277\tAverage Loss: 0.3824088768099559\n",
      "\tTest: Average Accuracy: 0.7932402482269503\tAverage Loss: 0.7684620695942719\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.8982875799573561\tAverage Loss: 0.3571997093206776\n",
      "\tTest: Average Accuracy: 0.7976507092198583\tAverage Loss: 0.756709770156858\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.9041677771855011\tAverage Loss: 0.3362411616334489\n",
      "\tTest: Average Accuracy: 0.8018838652482269\tAverage Loss: 0.7469093003413051\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.9095982142857143\tAverage Loss: 0.31750403836867874\n",
      "\tTest: Average Accuracy: 0.8031914893617021\tAverage Loss: 0.7437538042583891\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.9143789978678039\tAverage Loss: 0.30125061880483567\n",
      "\tTest: Average Accuracy: 0.8050531914893617\tAverage Loss: 0.7404198922140836\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.9185434434968017\tAverage Loss: 0.2868355091296193\n",
      "\tTest: Average Accuracy: 0.8077127659574468\tAverage Loss: 0.7397345626911729\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.9220082622601279\tAverage Loss: 0.2734729810359196\n",
      "\tTest: Average Accuracy: 0.8095079787234043\tAverage Loss: 0.7462930402622574\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.9254564232409381\tAverage Loss: 0.2613847096209748\n",
      "\tTest: Average Accuracy: 0.8105274822695034\tAverage Loss: 0.7486418538372331\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.9282216151385928\tAverage Loss: 0.25044909595079984\n",
      "\tTest: Average Accuracy: 0.8116356382978723\tAverage Loss: 0.7566730299869355\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.9309701492537313\tAverage Loss: 0.24011697695580675\n",
      "\tTest: Average Accuracy: 0.810372340425532\tAverage Loss: 0.7680809911229756\n"
     ]
    }
   ],
   "source": [
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of low learning rate\n",
    "We see the model doesn't evolve as fast and reaches less accuracy in the same amount of epochs also the model might be less general due to over fitting that comes with low LR and the test gets worst accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.056919642857142856\tAverage Loss: 2.9497576895331212\n",
      "\tTest: Average Accuracy: 0.05642730496453901\tAverage Loss: 2.94486745680227\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.06421575159914712\tAverage Loss: 2.9251157685923204\n",
      "\tTest: Average Accuracy: 0.07854609929078016\tAverage Loss: 2.9271291543551476\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.08207289445628999\tAverage Loss: 2.9035127078444134\n",
      "\tTest: Average Accuracy: 0.10252659574468086\tAverage Loss: 2.9075220528882766\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.12058568763326226\tAverage Loss: 2.858659138771639\n",
      "\tTest: Average Accuracy: 0.16006205673758864\tAverage Loss: 2.803919330594154\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.1965118603411514\tAverage Loss: 2.532341808483478\n",
      "\tTest: Average Accuracy: 0.1892730496453901\tAverage Loss: 2.4895827305414002\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.25631329957356075\tAverage Loss: 2.3180644282476752\n",
      "\tTest: Average Accuracy: 0.25851063829787235\tAverage Loss: 2.361201259421339\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.3662046908315565\tAverage Loss: 2.043034055910602\n",
      "\tTest: Average Accuracy: 0.35314716312056743\tAverage Loss: 2.1232390439208473\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.46293643390191896\tAverage Loss: 1.777307135502455\n",
      "\tTest: Average Accuracy: 0.39111258865248233\tAverage Loss: 2.047811078187053\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.5081623134328358\tAverage Loss: 1.6617469382540753\n",
      "\tTest: Average Accuracy: 0.42347074468085105\tAverage Loss: 1.989618671531039\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.5531716417910447\tAverage Loss: 1.5462243034419803\n",
      "\tTest: Average Accuracy: 0.46232269503546103\tAverage Loss: 1.8622167425862526\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.5967983742004265\tAverage Loss: 1.4000556690910402\n",
      "\tTest: Average Accuracy: 0.4947473404255319\tAverage Loss: 1.7350228797683516\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.6235674307036247\tAverage Loss: 1.2998355176468268\n",
      "\tTest: Average Accuracy: 0.5152703900709219\tAverage Loss: 1.6581242547018513\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.644422974413646\tAverage Loss: 1.2347547837315445\n",
      "\tTest: Average Accuracy: 0.5351063829787234\tAverage Loss: 1.6076903089860195\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.6605976812366737\tAverage Loss: 1.188389756464687\n",
      "\tTest: Average Accuracy: 0.5474512411347519\tAverage Loss: 1.5733645557240539\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.6723747334754797\tAverage Loss: 1.152449987272961\n",
      "\tTest: Average Accuracy: 0.5606382978723404\tAverage Loss: 1.5435512809146903\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.6832189498933902\tAverage Loss: 1.1190424252876277\n",
      "\tTest: Average Accuracy: 0.5711436170212766\tAverage Loss: 1.5127135344387594\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.6941797707889126\tAverage Loss: 1.0848378376134564\n",
      "\tTest: Average Accuracy: 0.5831117021276596\tAverage Loss: 1.4800301390248047\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.7035247867803838\tAverage Loss: 1.0529088127193538\n",
      "\tTest: Average Accuracy: 0.5930629432624114\tAverage Loss: 1.4510673197154924\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.712869802771855\tAverage Loss: 1.0260769325733494\n",
      "\tTest: Average Accuracy: 0.602238475177305\tAverage Loss: 1.4255453207287163\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.7196162046908315\tAverage Loss: 1.003097853607801\n",
      "\tTest: Average Accuracy: 0.610549645390071\tAverage Loss: 1.4016615864820219\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=1e-4)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of high learning rate\n",
    "This time due to big step sizes the model over shoots from were it should be and never really finds the optimal weights.<br>\n",
    "as we see below the model doesn't actualy get trained at all and the learning starts to feel like a random walk of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.05563699360341151\tAverage Loss: 2.9850127862554103\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950337314937101\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516135413028857\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334727651935\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134269419716\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.95033470460403\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.951613426095616\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.9503347043740984\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.951613426087192\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371802\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.951613426087107\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.95033470437178\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.05553704690831557\tAverage Loss: 2.9516134260871065\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.950334704371779\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=1e-2)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing some other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.11470549040511727\tAverage Loss: 2.78879364740919\n",
      "\tTest: Average Accuracy: 0.16484929078014182\tAverage Loss: 2.5141686939214796\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.3369369669509595\tAverage Loss: 2.075351329244754\n",
      "\tTest: Average Accuracy: 0.4403812056737589\tAverage Loss: 1.8935078137266426\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.6135061300639659\tAverage Loss: 1.3157172630138343\n",
      "\tTest: Average Accuracy: 0.5425975177304965\tAverage Loss: 1.5662541679205382\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.6733408848614072\tAverage Loss: 1.1225720744793712\n",
      "\tTest: Average Accuracy: 0.5841090425531915\tAverage Loss: 1.4165309210660806\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.7224480277185501\tAverage Loss: 0.959384732798132\n",
      "\tTest: Average Accuracy: 0.6301418439716311\tAverage Loss: 1.2774583946721834\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.7556969616204691\tAverage Loss: 0.8445645236576788\n",
      "\tTest: Average Accuracy: 0.6609485815602838\tAverage Loss: 1.198572124776642\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.7782349413646056\tAverage Loss: 0.7722855386228664\n",
      "\tTest: Average Accuracy: 0.6822916666666667\tAverage Loss: 1.1400610378695883\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.7965418443496801\tAverage Loss: 0.7156427162354746\n",
      "\tTest: Average Accuracy: 0.6962544326241136\tAverage Loss: 1.0954908974021897\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.8099347014925373\tAverage Loss: 0.6697601118579818\n",
      "\tTest: Average Accuracy: 0.7050975177304966\tAverage Loss: 1.0553658521529905\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.8219782782515992\tAverage Loss: 0.6316486944708919\n",
      "\tTest: Average Accuracy: 0.717331560283688\tAverage Loss: 1.0140093474136767\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.8316731076759062\tAverage Loss: 0.5990261203394105\n",
      "\tTest: Average Accuracy: 0.7283466312056737\tAverage Loss: 0.9788634535759815\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.8390691631130064\tAverage Loss: 0.5706559219908238\n",
      "\tTest: Average Accuracy: 0.7358599290780141\tAverage Loss: 0.9447771471863099\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.8468150319829424\tAverage Loss: 0.5442552100761705\n",
      "\tTest: Average Accuracy: 0.7441046099290779\tAverage Loss: 0.9185635746401837\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.8531449893390192\tAverage Loss: 0.5201783264648054\n",
      "\tTest: Average Accuracy: 0.7524157801418438\tAverage Loss: 0.893100935662078\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.8598747334754797\tAverage Loss: 0.497573918934594\n",
      "\tTest: Average Accuracy: 0.7602171985815603\tAverage Loss: 0.868127142986731\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.865338486140725\tAverage Loss: 0.4766188731984448\n",
      "\tTest: Average Accuracy: 0.7664007092198583\tAverage Loss: 0.8463055968139536\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.8706689765458422\tAverage Loss: 0.4568975151394974\n",
      "\tTest: Average Accuracy: 0.7727836879432625\tAverage Loss: 0.8262428546047358\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.8767990405117271\tAverage Loss: 0.43834112220535093\n",
      "\tTest: Average Accuracy: 0.7779698581560285\tAverage Loss: 0.80941107286283\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.8819629530916845\tAverage Loss: 0.42123436991138263\n",
      "\tTest: Average Accuracy: 0.78167109929078\tAverage Loss: 0.7961943163159348\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.88667710554371\tAverage Loss: 0.4051880193025172\n",
      "\tTest: Average Accuracy: 0.7860593971631205\tAverage Loss: 0.7826153945918181\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=5e-4)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.15401785714285715\tAverage Loss: 2.624897860980105\n",
      "\tTest: Average Accuracy: 0.23331117021276596\tAverage Loss: 2.3029445716655386\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.5179071162046909\tAverage Loss: 1.5731218638024467\n",
      "\tTest: Average Accuracy: 0.5204565602836879\tAverage Loss: 1.6388090447812562\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.6759894722814499\tAverage Loss: 1.1052371697005514\n",
      "\tTest: Average Accuracy: 0.6038785460992907\tAverage Loss: 1.354206937523254\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.7445862206823027\tAverage Loss: 0.8883832628610179\n",
      "\tTest: Average Accuracy: 0.6662455673758865\tAverage Loss: 1.1834640967451175\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.7865138592750534\tAverage Loss: 0.7551030258548472\n",
      "\tTest: Average Accuracy: 0.6937721631205673\tAverage Loss: 1.08224744723786\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.8103844616204691\tAverage Loss: 0.6701360678062943\n",
      "\tTest: Average Accuracy: 0.714450354609929\tAverage Loss: 1.0057792270933488\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.830007329424307\tAverage Loss: 0.6045486506709887\n",
      "\tTest: Average Accuracy: 0.7343306737588652\tAverage Loss: 0.9444224292859187\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.8445828891257996\tAverage Loss: 0.5526987631242221\n",
      "\tTest: Average Accuracy: 0.7470079787234043\tAverage Loss: 0.8997138511106701\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.8551106076759062\tAverage Loss: 0.5111455490739598\n",
      "\tTest: Average Accuracy: 0.7584441489361702\tAverage Loss: 0.8610172987724378\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.8650719616204691\tAverage Loss: 0.4757154952786591\n",
      "\tTest: Average Accuracy: 0.7679299645390072\tAverage Loss: 0.8308662422649329\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.8736840351812367\tAverage Loss: 0.4449637227304593\n",
      "\tTest: Average Accuracy: 0.7772828014184396\tAverage Loss: 0.8023643521490283\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.8815631663113006\tAverage Loss: 0.41805511136947804\n",
      "\tTest: Average Accuracy: 0.7832668439716312\tAverage Loss: 0.7839705926474959\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.8890591684434968\tAverage Loss: 0.3944134010749174\n",
      "\tTest: Average Accuracy: 0.78792109929078\tAverage Loss: 0.7651004011597834\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.8956056769722814\tAverage Loss: 0.37334384023606537\n",
      "\tTest: Average Accuracy: 0.7934397163120567\tAverage Loss: 0.7510303582416333\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.9009694829424307\tAverage Loss: 0.35466221575799745\n",
      "\tTest: Average Accuracy: 0.7984264184397162\tAverage Loss: 0.7392480010519965\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.9048507462686567\tAverage Loss: 0.33825213727331327\n",
      "\tTest: Average Accuracy: 0.8018838652482269\tAverage Loss: 0.7309359635559141\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.9088819296375267\tAverage Loss: 0.3232686363398863\n",
      "\tTest: Average Accuracy: 0.8046764184397163\tAverage Loss: 0.7239485387986548\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.9129464285714286\tAverage Loss: 0.3097006711740632\n",
      "\tTest: Average Accuracy: 0.807668439716312\tAverage Loss: 0.7221909070418485\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.916028118336887\tAverage Loss: 0.2971946221867208\n",
      "\tTest: Average Accuracy: 0.8095744680851064\tAverage Loss: 0.7193324172001009\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.9191930970149254\tAverage Loss: 0.28572167109336355\n",
      "\tTest: Average Accuracy: 0.8105053191489362\tAverage Loss: 0.7207219569629201\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=Relu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=8e-4)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we saw above the best learning rate is achived with lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.05695295842217484\tAverage Loss: 2.9549450801663477\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.952835374565426\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.05706956289978678\tAverage Loss: 2.95324499855623\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.9519365898800283\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.05783582089552239\tAverage Loss: 2.952296942461798\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.9510485774713247\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.05873534115138593\tAverage Loss: 2.9512818152726723\n",
      "\tTest: Average Accuracy: 0.05680407801418439\tAverage Loss: 2.9499788818405754\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.062316764392324094\tAverage Loss: 2.9493943421496738\n",
      "\tTest: Average Accuracy: 0.057867907801418435\tAverage Loss: 2.9476840147082646\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.07192830490405118\tAverage Loss: 2.9450482031524494\n",
      "\tTest: Average Accuracy: 0.07706117021276596\tAverage Loss: 2.942838105444\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.09055170575692964\tAverage Loss: 2.9349473242097908\n",
      "\tTest: Average Accuracy: 0.0831781914893617\tAverage Loss: 2.9301322619866554\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.10834221748400853\tAverage Loss: 2.8832949129838115\n",
      "\tTest: Average Accuracy: 0.09594414893617022\tAverage Loss: 2.819959315147928\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.12846481876332622\tAverage Loss: 2.6670988457598117\n",
      "\tTest: Average Accuracy: 0.12058953900709218\tAverage Loss: 2.623153340280107\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.16627798507462688\tAverage Loss: 2.487542051859279\n",
      "\tTest: Average Accuracy: 0.13455230496453902\tAverage Loss: 2.499679356762083\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.19464618869936035\tAverage Loss: 2.370406669910037\n",
      "\tTest: Average Accuracy: 0.16389627659574468\tAverage Loss: 2.425878344840601\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.21613472814498935\tAverage Loss: 2.2933923363678463\n",
      "\tTest: Average Accuracy: 0.19827127659574467\tAverage Loss: 2.372211796700418\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.23773987206823027\tAverage Loss: 2.230955065065891\n",
      "\tTest: Average Accuracy: 0.22085549645390073\tAverage Loss: 2.3273274859376296\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.26146055437100213\tAverage Loss: 2.1655121536662683\n",
      "\tTest: Average Accuracy: 0.24177748226950352\tAverage Loss: 2.2724087055085813\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.30437100213219614\tAverage Loss: 2.06628395017011\n",
      "\tTest: Average Accuracy: 0.30611702127659574\tAverage Loss: 2.173969379693353\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.3920742270788913\tAverage Loss: 1.907943003682336\n",
      "\tTest: Average Accuracy: 0.35738031914893614\tAverage Loss: 2.05260004502189\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.4427472014925373\tAverage Loss: 1.7460122265154225\n",
      "\tTest: Average Accuracy: 0.39871453900709214\tAverage Loss: 1.9608129245156432\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.49210421108742003\tAverage Loss: 1.621593991449836\n",
      "\tTest: Average Accuracy: 0.43036347517730494\tAverage Loss: 1.8929770224483837\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.5345315831556503\tAverage Loss: 1.5258058056342794\n",
      "\tTest: Average Accuracy: 0.45833333333333337\tAverage Loss: 1.8356607852592004\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.5769422974413646\tAverage Loss: 1.443280921123597\n",
      "\tTest: Average Accuracy: 0.48490691489361704\tAverage Loss: 1.778444701182304\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=Sigmoid())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=Sigmoid())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.05733608742004265\tAverage Loss: 2.954980157415014\n",
      "\tTest: Average Accuracy: 0.0570700354609929\tAverage Loss: 2.9519034532839563\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.07202825159914712\tAverage Loss: 2.9437087975871985\n",
      "\tTest: Average Accuracy: 0.08410904255319149\tAverage Loss: 2.905080945160202\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.18688366204690832\tAverage Loss: 2.4538556371334383\n",
      "\tTest: Average Accuracy: 0.22457890070921988\tAverage Loss: 2.3138728060168106\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.3628897921108742\tAverage Loss: 1.8988172440918751\n",
      "\tTest: Average Accuracy: 0.3745345744680851\tAverage Loss: 1.9550493949714298\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.5364972014925373\tAverage Loss: 1.476832808453719\n",
      "\tTest: Average Accuracy: 0.49629875886524827\tAverage Loss: 1.6923783945070654\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.6435234541577826\tAverage Loss: 1.2095827275425075\n",
      "\tTest: Average Accuracy: 0.5498005319148936\tAverage Loss: 1.536697273247684\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.6873334221748401\tAverage Loss: 1.0673410512629002\n",
      "\tTest: Average Accuracy: 0.5845301418439716\tAverage Loss: 1.4325204517047554\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.7222647921108742\tAverage Loss: 0.9612181872787295\n",
      "\tTest: Average Accuracy: 0.6156914893617021\tAverage Loss: 1.3453798643697106\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.7489838752665245\tAverage Loss: 0.8724338553242437\n",
      "\tTest: Average Accuracy: 0.6324689716312056\tAverage Loss: 1.2753259670141117\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.767590618336887\tAverage Loss: 0.8057221409694105\n",
      "\tTest: Average Accuracy: 0.651529255319149\tAverage Loss: 1.2078635702094178\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.7844816098081023\tAverage Loss: 0.751369473506092\n",
      "\tTest: Average Accuracy: 0.6678191489361702\tAverage Loss: 1.1529307349740805\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.7976912313432836\tAverage Loss: 0.7071627728376149\n",
      "\tTest: Average Accuracy: 0.6791666666666667\tAverage Loss: 1.1171446980331232\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.8064032515991472\tAverage Loss: 0.6737996408102574\n",
      "\tTest: Average Accuracy: 0.6860150709219859\tAverage Loss: 1.0964139979592649\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.8148820628997868\tAverage Loss: 0.6465969619704068\n",
      "\tTest: Average Accuracy: 0.6919547872340426\tAverage Loss: 1.0794924613433172\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.8227778518123667\tAverage Loss: 0.6226726702893141\n",
      "\tTest: Average Accuracy: 0.7\tAverage Loss: 1.0632909657913538\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.8297241471215352\tAverage Loss: 0.6009242767404676\n",
      "\tTest: Average Accuracy: 0.7059840425531915\tAverage Loss: 1.0494311016304487\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.8352212153518124\tAverage Loss: 0.5806552711822425\n",
      "\tTest: Average Accuracy: 0.7109929078014183\tAverage Loss: 1.0362475646492242\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.8410847547974414\tAverage Loss: 0.5606768535964045\n",
      "\tTest: Average Accuracy: 0.7187278368794328\tAverage Loss: 1.0214430567137045\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.8463152985074627\tAverage Loss: 0.5418764812494717\n",
      "\tTest: Average Accuracy: 0.7244902482269503\tAverage Loss: 1.0073660690588042\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.8509794776119403\tAverage Loss: 0.5251454808735093\n",
      "\tTest: Average Accuracy: 0.7281914893617021\tAverage Loss: 0.9953073928403957\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=Tanh())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=Tanh())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages of sigmoid and tanh\n",
    "They have the exploding gradient and vanishing gradient problem in witch the gradient is eigher too big or too small. this is less of a problem with tanh.<br>\n",
    "They are computationally more expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.14717150852878466\tAverage Loss: 2.629058520001742\n",
      "\tTest: Average Accuracy: 0.21549202127659575\tAverage Loss: 2.316292620805588\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.45355810234541577\tAverage Loss: 1.7583132038991027\n",
      "\tTest: Average Accuracy: 0.47050088652482275\tAverage Loss: 1.8150635264381845\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.6058435501066098\tAverage Loss: 1.327156124825816\n",
      "\tTest: Average Accuracy: 0.545168439716312\tAverage Loss: 1.5357670335636124\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.7137693230277186\tAverage Loss: 0.9938127681878438\n",
      "\tTest: Average Accuracy: 0.652061170212766\tAverage Loss: 1.2377843654066563\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.7808668710021321\tAverage Loss: 0.7738259001227596\n",
      "\tTest: Average Accuracy: 0.6978945035460992\tAverage Loss: 1.068268218071258\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.820295842217484\tAverage Loss: 0.6370564280465667\n",
      "\tTest: Average Accuracy: 0.7331338652482269\tAverage Loss: 0.9428553235821449\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.8479144456289979\tAverage Loss: 0.5410905011254935\n",
      "\tTest: Average Accuracy: 0.7593971631205673\tAverage Loss: 0.84957809854304\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.8662380063965884\tAverage Loss: 0.47560134978919094\n",
      "\tTest: Average Accuracy: 0.7761081560283689\tAverage Loss: 0.7911312835651337\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.87894789445629\tAverage Loss: 0.42780640165123474\n",
      "\tTest: Average Accuracy: 0.7890070921985817\tAverage Loss: 0.7498897728247679\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.8895755597014925\tAverage Loss: 0.39032146746886204\n",
      "\tTest: Average Accuracy: 0.7965868794326242\tAverage Loss: 0.721825013752899\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.8979544243070362\tAverage Loss: 0.3603525580461165\n",
      "\tTest: Average Accuracy: 0.8041445035460992\tAverage Loss: 0.6994432479995956\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.9051672441364605\tAverage Loss: 0.3360699405664308\n",
      "\tTest: Average Accuracy: 0.8085549645390072\tAverage Loss: 0.6864798681384385\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.9104977345415778\tAverage Loss: 0.3160165811088968\n",
      "\tTest: Average Accuracy: 0.8122118794326242\tAverage Loss: 0.6768512177982329\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.9148953891257996\tAverage Loss: 0.29893837048075206\n",
      "\tTest: Average Accuracy: 0.8142508865248226\tAverage Loss: 0.673141487009301\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.919976012793177\tAverage Loss: 0.2840131421595958\n",
      "\tTest: Average Accuracy: 0.8166445035460992\tAverage Loss: 0.6718192669483625\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.9236407249466951\tAverage Loss: 0.2706883224153413\n",
      "\tTest: Average Accuracy: 0.8188386524822694\tAverage Loss: 0.6703695670097206\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.9268390191897654\tAverage Loss: 0.25851728163640436\n",
      "\tTest: Average Accuracy: 0.8209663120567375\tAverage Loss: 0.6722678747152813\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.9302372068230277\tAverage Loss: 0.2473659053126463\n",
      "\tTest: Average Accuracy: 0.8209663120567375\tAverage Loss: 0.6777155756307833\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.9331856343283582\tAverage Loss: 0.23737282521594363\n",
      "\tTest: Average Accuracy: 0.8222296099290779\tAverage Loss: 0.6809874293971664\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.9357842484008528\tAverage Loss: 0.2284026534634797\n",
      "\tTest: Average Accuracy: 0.8230939716312056\tAverage Loss: 0.6898925236967393\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of leaky relu\n",
    "1) trys to fix the dying relu problem in witch for negative values the weights are not updated<br>\n",
    "2) is faster at learning because of being more balanced(effect of the first point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.19376666666666667\tAverage Loss: 2.501821634222561\n",
      "\tTest: Average Accuracy: 0.38093017057569295\tAverage Loss: 2.036706635359228\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.6091\tAverage Loss: 1.3288536711303083\n",
      "\tTest: Average Accuracy: 0.5835554371002132\tAverage Loss: 1.4604937396846942\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.7297166666666667\tAverage Loss: 0.9467823594852111\n",
      "\tTest: Average Accuracy: 0.6568496801705757\tAverage Loss: 1.205537845299422\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.7753166666666667\tAverage Loss: 0.7817835179170426\n",
      "\tTest: Average Accuracy: 0.6918976545842217\tAverage Loss: 1.0913962196750275\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.8058333333333333\tAverage Loss: 0.6787159500302257\n",
      "\tTest: Average Accuracy: 0.7191497867803838\tAverage Loss: 1.0008969958715794\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.8281666666666667\tAverage Loss: 0.6002395992233253\n",
      "\tTest: Average Accuracy: 0.7378731343283582\tAverage Loss: 0.9297255080114577\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.8468666666666667\tAverage Loss: 0.5364976567589509\n",
      "\tTest: Average Accuracy: 0.7559968017057569\tAverage Loss: 0.8680874484289345\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.86265\tAverage Loss: 0.4840075904939071\n",
      "\tTest: Average Accuracy: 0.7705223880597015\tAverage Loss: 0.8142262231969397\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.8744166666666666\tAverage Loss: 0.44145570216911995\n",
      "\tTest: Average Accuracy: 0.7824493603411514\tAverage Loss: 0.7777674801573705\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.88455\tAverage Loss: 0.40691758504333764\n",
      "\tTest: Average Accuracy: 0.7907116204690832\tAverage Loss: 0.746894444446282\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.8927166666666667\tAverage Loss: 0.3781861465412978\n",
      "\tTest: Average Accuracy: 0.7966417910447762\tAverage Loss: 0.724920598804074\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.8993333333333333\tAverage Loss: 0.3537224767992401\n",
      "\tTest: Average Accuracy: 0.8039712153518124\tAverage Loss: 0.7061735856675783\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.90565\tAverage Loss: 0.33271398500686944\n",
      "\tTest: Average Accuracy: 0.8076359275053305\tAverage Loss: 0.6938031023959649\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.9105333333333333\tAverage Loss: 0.31448089150735176\n",
      "\tTest: Average Accuracy: 0.8108342217484008\tAverage Loss: 0.6865634238673561\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.9149333333333334\tAverage Loss: 0.29886726472851816\n",
      "\tTest: Average Accuracy: 0.8130996801705757\tAverage Loss: 0.6855778484750551\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.9190833333333334\tAverage Loss: 0.2847401243128325\n",
      "\tTest: Average Accuracy: 0.8133662046908315\tAverage Loss: 0.6856938072314402\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.92315\tAverage Loss: 0.27165936993821216\n",
      "\tTest: Average Accuracy: 0.8146988272921108\tAverage Loss: 0.6882991727913373\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.9265166666666667\tAverage Loss: 0.2601192814980744\n",
      "\tTest: Average Accuracy: 0.8180970149253731\tAverage Loss: 0.6901671851632318\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.9294\tAverage Loss: 0.24980007302937005\n",
      "\tTest: Average Accuracy: 0.8194962686567164\tAverage Loss: 0.696176966616631\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.93245\tAverage Loss: 0.23987066974020843\n",
      "\tTest: Average Accuracy: 0.8209621535181236\tAverage Loss: 0.6953699590054075\n"
     ]
    }
   ],
   "source": [
    "trainloader = Dataloader(X_train, y_train, n_classes=20, batch_size=16)\n",
    "testloader = Dataloader(X_test, y_test, n_classes=20, batch_size=16)\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.1948\tAverage Loss: 2.4973813043835915\n",
      "\tTest: Average Accuracy: 0.3988539445628998\tAverage Loss: 1.9613016453380305\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.6276166666666667\tAverage Loss: 1.2745265251769948\n",
      "\tTest: Average Accuracy: 0.5877309879175551\tAverage Loss: 1.4029012968074612\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.7333\tAverage Loss: 0.9191459518888985\n",
      "\tTest: Average Accuracy: 0.6605810234541578\tAverage Loss: 1.1817531408933435\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.7828666666666667\tAverage Loss: 0.7586987964858503\n",
      "\tTest: Average Accuracy: 0.693430170575693\tAverage Loss: 1.0760230583798294\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.8100333333333334\tAverage Loss: 0.6617884416978734\n",
      "\tTest: Average Accuracy: 0.7193718905472637\tAverage Loss: 0.9898261479058199\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.83035\tAverage Loss: 0.589197904169067\n",
      "\tTest: Average Accuracy: 0.740227434257285\tAverage Loss: 0.9218676084134331\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.84795\tAverage Loss: 0.5296817015337907\n",
      "\tTest: Average Accuracy: 0.7561522743425729\tAverage Loss: 0.862600064932003\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.8609\tAverage Loss: 0.48020571202557716\n",
      "\tTest: Average Accuracy: 0.7694118692253021\tAverage Loss: 0.8164234727233304\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.8726666666666667\tAverage Loss: 0.4392438096518599\n",
      "\tTest: Average Accuracy: 0.7829157782515992\tAverage Loss: 0.7763316498641052\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.8825833333333334\tAverage Loss: 0.4050354210878154\n",
      "\tTest: Average Accuracy: 0.7920442430703625\tAverage Loss: 0.7466795463251643\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.8911666666666667\tAverage Loss: 0.3758759159229096\n",
      "\tTest: Average Accuracy: 0.7965085287846482\tAverage Loss: 0.7308675441738467\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.8984666666666666\tAverage Loss: 0.3514834731086854\n",
      "\tTest: Average Accuracy: 0.8031050106609808\tAverage Loss: 0.7155971292470723\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.9042666666666667\tAverage Loss: 0.3310508174357647\n",
      "\tTest: Average Accuracy: 0.8083022388059702\tAverage Loss: 0.706044987325309\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.909\tAverage Loss: 0.31376703623552055\n",
      "\tTest: Average Accuracy: 0.808346659559346\tAverage Loss: 0.7062742995731107\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.91405\tAverage Loss: 0.2984511585587071\n",
      "\tTest: Average Accuracy: 0.8106787491115849\tAverage Loss: 0.7024935417773763\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.9177833333333333\tAverage Loss: 0.28436663634222337\n",
      "\tTest: Average Accuracy: 0.8121668443496801\tAverage Loss: 0.7006588431130545\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.9221\tAverage Loss: 0.27174773750549586\n",
      "\tTest: Average Accuracy: 0.8159648187633263\tAverage Loss: 0.6965258273622299\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.9255333333333333\tAverage Loss: 0.260180296854261\n",
      "\tTest: Average Accuracy: 0.8174307036247335\tAverage Loss: 0.7004904530581229\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.9287\tAverage Loss: 0.24975882423455795\n",
      "\tTest: Average Accuracy: 0.8176972281449894\tAverage Loss: 0.7056391352861522\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.9310333333333334\tAverage Loss: 0.24053402151689374\n",
      "\tTest: Average Accuracy: 0.8173418621179814\tAverage Loss: 0.7138732720196121\n"
     ]
    }
   ],
   "source": [
    "trainloader = Dataloader(X_train, y_train, n_classes=20, batch_size=32)\n",
    "testloader = Dataloader(X_test, y_test, n_classes=20, batch_size=32)\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.07275044326241134\tAverage Loss: 2.9882847257049563\n",
      "\tTest: Average Accuracy: 0.093966045941124\tAverage Loss: 2.9047315037401455\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.10753546099290781\tAverage Loss: 2.8706334740677892\n",
      "\tTest: Average Accuracy: 0.08546707738626226\tAverage Loss: 2.903753348788812\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.09563940602836879\tAverage Loss: 2.874389104584136\n",
      "\tTest: Average Accuracy: 0.07577637154326494\tAverage Loss: 2.9244645618023815\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.08384862588652484\tAverage Loss: 2.905621484022401\n",
      "\tTest: Average Accuracy: 0.06277528434433541\tAverage Loss: 2.95737293613994\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.05597850177304965\tAverage Loss: 2.9705032675477594\n",
      "\tTest: Average Accuracy: 0.04458560994647636\tAverage Loss: 2.995058288535662\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.045944148936170215\tAverage Loss: 2.993959865910819\n",
      "\tTest: Average Accuracy: 0.043815510704727924\tAverage Loss: 2.9678989596066945\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.05853280141843971\tAverage Loss: 2.9403165164844016\n",
      "\tTest: Average Accuracy: 0.06463258251561106\tAverage Loss: 2.8973878777763544\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.10212765957446808\tAverage Loss: 2.8005218030463195\n",
      "\tTest: Average Accuracy: 0.13204588537020517\tAverage Loss: 2.739373497281709\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.2232989804964539\tAverage Loss: 2.4145505723541203\n",
      "\tTest: Average Accuracy: 0.23743448929527208\tAverage Loss: 2.383730024695597\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.4022218528368794\tAverage Loss: 1.8898365364875707\n",
      "\tTest: Average Accuracy: 0.4302589763603925\tAverage Loss: 1.8380010502002466\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.5950520833333333\tAverage Loss: 1.3216259868064923\n",
      "\tTest: Average Accuracy: 0.5574821587867975\tAverage Loss: 1.500487932993218\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.6874113475177306\tAverage Loss: 1.0394468273839448\n",
      "\tTest: Average Accuracy: 0.6322514774754683\tAverage Loss: 1.3056394562156188\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.7476063829787234\tAverage Loss: 0.8661070689435844\n",
      "\tTest: Average Accuracy: 0.6570410069134701\tAverage Loss: 1.2138895176746376\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.7712101063829787\tAverage Loss: 0.7781287212362413\n",
      "\tTest: Average Accuracy: 0.6750459968777877\tAverage Loss: 1.1557181718135192\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.7882147606382979\tAverage Loss: 0.7193128059992271\n",
      "\tTest: Average Accuracy: 0.6907650814005353\tAverage Loss: 1.1054101556464386\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.801989140070922\tAverage Loss: 0.6701170696961533\n",
      "\tTest: Average Accuracy: 0.7030587923728814\tAverage Loss: 1.061778577372031\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.814982269503546\tAverage Loss: 0.6282725065553446\n",
      "\tTest: Average Accuracy: 0.7125926906779662\tAverage Loss: 1.025224978630082\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.824501329787234\tAverage Loss: 0.5950338332173366\n",
      "\tTest: Average Accuracy: 0.7186210693577163\tAverage Loss: 0.9982342584207046\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.8327016843971631\tAverage Loss: 0.568185060801358\n",
      "\tTest: Average Accuracy: 0.7276253066458519\tAverage Loss: 0.9697976781518401\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.8387854609929077\tAverage Loss: 0.5454724977073784\n",
      "\tTest: Average Accuracy: 0.7336955006690455\tAverage Loss: 0.9516998406554238\n"
     ]
    }
   ],
   "source": [
    "trainloader = Dataloader(X_train, y_train, n_classes=20, batch_size=256)\n",
    "testloader = Dataloader(X_test, y_test, n_classes=20, batch_size=256)\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference\n",
    "As we see smaller batches lead to more passes through the network so we have more fitting per epoch also this makes the prosses so slower. <br>\n",
    "As we see bigger the network gets smaller batch size it will need because if I make the network smaller 256 batch size will yield a result.\n",
    "\n",
    "### Why batch\n",
    "We use batches to preserve memory because most data sets will have problems for small memories so we use batches to solve this problem.<br>\n",
    "64 was the better of the bunch for the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of epochs\n",
    "As the prosses is itterative and we have limited amount of training data we will itterate each record of training data more than one witch is the number of epochs.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 0.12898121002132196\tAverage Loss: 2.6800198987617074\n",
      "\tTest: Average Accuracy: 0.16992464539007093\tAverage Loss: 2.5127827542577803\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 0.2818330223880597\tAverage Loss: 2.185375973756346\n",
      "\tTest: Average Accuracy: 0.3640292553191489\tAverage Loss: 2.1716401831340746\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 0.5093616737739872\tAverage Loss: 1.6481792593502513\n",
      "\tTest: Average Accuracy: 0.49295212765957447\tAverage Loss: 1.758758642096895\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 0.6664112473347548\tAverage Loss: 1.1375240252374081\n",
      "\tTest: Average Accuracy: 0.6001551418439716\tAverage Loss: 1.4056006664669018\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 0.7445029317697228\tAverage Loss: 0.8885056863558424\n",
      "\tTest: Average Accuracy: 0.6756870567375886\tAverage Loss: 1.1590973252816954\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 0.8054870735607675\tAverage Loss: 0.6857275044328173\n",
      "\tTest: Average Accuracy: 0.7192597517730497\tAverage Loss: 0.9984731264355623\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 0.8389692164179104\tAverage Loss: 0.5705686558820856\n",
      "\tTest: Average Accuracy: 0.7474512411347518\tAverage Loss: 0.8994312545265722\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 0.8602911780383795\tAverage Loss: 0.496193608982715\n",
      "\tTest: Average Accuracy: 0.7653590425531915\tAverage Loss: 0.8414664011841577\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 0.8734841417910447\tAverage Loss: 0.44364434359684335\n",
      "\tTest: Average Accuracy: 0.7771054964539008\tAverage Loss: 0.8006247119659341\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 0.8840118603411514\tAverage Loss: 0.4051539554362\n",
      "\tTest: Average Accuracy: 0.7840647163120567\tAverage Loss: 0.7756602038438768\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 0.892557302771855\tAverage Loss: 0.375686959357272\n",
      "\tTest: Average Accuracy: 0.7903590425531914\tAverage Loss: 0.7617342960207135\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 0.8994869402985075\tAverage Loss: 0.35103213384655946\n",
      "\tTest: Average Accuracy: 0.7950354609929077\tAverage Loss: 0.7515128707689773\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 0.9060834221748401\tAverage Loss: 0.32988863251175676\n",
      "\tTest: Average Accuracy: 0.7994902482269503\tAverage Loss: 0.7439793356379971\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 0.9108808635394456\tAverage Loss: 0.31209816684004804\n",
      "\tTest: Average Accuracy: 0.8029476950354609\tAverage Loss: 0.7389043590733105\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 0.9159281716417911\tAverage Loss: 0.29602512937740083\n",
      "\tTest: Average Accuracy: 0.8072030141843971\tAverage Loss: 0.7278651948522192\n",
      "Epoch 16:\n",
      "\tTrain: Average Accuracy: 0.9197927771855011\tAverage Loss: 0.281490965908985\n",
      "\tTest: Average Accuracy: 0.8092641843971631\tAverage Loss: 0.7271609204797066\n",
      "Epoch 17:\n",
      "\tTrain: Average Accuracy: 0.9232076226012793\tAverage Loss: 0.2689124573107576\n",
      "\tTest: Average Accuracy: 0.8116578014184397\tAverage Loss: 0.7266114708989354\n",
      "Epoch 18:\n",
      "\tTrain: Average Accuracy: 0.9266557835820896\tAverage Loss: 0.25725422063970593\n",
      "\tTest: Average Accuracy: 0.8130540780141843\tAverage Loss: 0.7283318373010125\n",
      "Epoch 19:\n",
      "\tTrain: Average Accuracy: 0.9295042643923241\tAverage Loss: 0.24642960951741347\n",
      "\tTest: Average Accuracy: 0.811857269503546\tAverage Loss: 0.7376728111137977\n",
      "Epoch 20:\n",
      "\tTrain: Average Accuracy: 0.9320695628997868\tAverage Loss: 0.23635326254372868\n",
      "\tTest: Average Accuracy: 0.8126994680851064\tAverage Loss: 0.7430506131477291\n",
      "Epoch 21:\n",
      "\tTrain: Average Accuracy: 0.9349680170575693\tAverage Loss: 0.22656133963909558\n",
      "\tTest: Average Accuracy: 0.8142508865248226\tAverage Loss: 0.7473866156370046\n",
      "Epoch 22:\n",
      "\tTrain: Average Accuracy: 0.9377165511727079\tAverage Loss: 0.21773634669669373\n",
      "\tTest: Average Accuracy: 0.8142952127659574\tAverage Loss: 0.752755034996345\n",
      "Epoch 23:\n",
      "\tTrain: Average Accuracy: 0.9403651385927505\tAverage Loss: 0.20910746855400464\n",
      "\tTest: Average Accuracy: 0.8154920212765957\tAverage Loss: 0.7596018332603695\n",
      "Epoch 24:\n",
      "\tTrain: Average Accuracy: 0.9430137260127932\tAverage Loss: 0.20151028865232093\n",
      "\tTest: Average Accuracy: 0.815625\tAverage Loss: 0.7704430754823884\n",
      "Epoch 25:\n",
      "\tTrain: Average Accuracy: 0.9451459221748401\tAverage Loss: 0.19375725800143812\n",
      "\tTest: Average Accuracy: 0.8160904255319149\tAverage Loss: 0.7793708192128846\n",
      "Epoch 26:\n",
      "\tTrain: Average Accuracy: 0.9473447494669509\tAverage Loss: 0.18708013258343156\n",
      "\tTest: Average Accuracy: 0.816688829787234\tAverage Loss: 0.7883361558952396\n",
      "Epoch 27:\n",
      "\tTrain: Average Accuracy: 0.9500599680170576\tAverage Loss: 0.18046837735615834\n",
      "\tTest: Average Accuracy: 0.8159796099290779\tAverage Loss: 0.8012517489572747\n",
      "Epoch 28:\n",
      "\tTrain: Average Accuracy: 0.9515924840085288\tAverage Loss: 0.1743110483614918\n",
      "\tTest: Average Accuracy: 0.8151152482269503\tAverage Loss: 0.8147736562706481\n",
      "Epoch 29:\n",
      "\tTrain: Average Accuracy: 0.9539578891257996\tAverage Loss: 0.16794348460477507\n",
      "\tTest: Average Accuracy: 0.8153147163120567\tAverage Loss: 0.8252163650309442\n",
      "Epoch 30:\n",
      "\tTrain: Average Accuracy: 0.9559235074626866\tAverage Loss: 0.16204262265479663\n",
      "\tTest: Average Accuracy: 0.8152482269503545\tAverage Loss: 0.8424186384590499\n",
      "Epoch 31:\n",
      "\tTrain: Average Accuracy: 0.95707289445629\tAverage Loss: 0.15668498746350285\n",
      "\tTest: Average Accuracy: 0.813918439716312\tAverage Loss: 0.8583218316222483\n",
      "Epoch 32:\n",
      "\tTrain: Average Accuracy: 0.9593550106609808\tAverage Loss: 0.1514405065969592\n",
      "\tTest: Average Accuracy: 0.8141843971631205\tAverage Loss: 0.8728944711056752\n",
      "Epoch 33:\n",
      "\tTrain: Average Accuracy: 0.9615538379530917\tAverage Loss: 0.1459671486151828\n",
      "\tTest: Average Accuracy: 0.8127216312056736\tAverage Loss: 0.8980622936678815\n",
      "Epoch 34:\n",
      "\tTrain: Average Accuracy: 0.962853144989339\tAverage Loss: 0.14133420456008128\n",
      "\tTest: Average Accuracy: 0.8112588652482269\tAverage Loss: 0.9137361856877643\n",
      "Epoch 35:\n",
      "\tTrain: Average Accuracy: 0.9647521321961621\tAverage Loss: 0.13652165232100003\n",
      "\tTest: Average Accuracy: 0.8114361702127659\tAverage Loss: 0.9338922529871401\n",
      "Epoch 36:\n",
      "\tTrain: Average Accuracy: 0.9659015191897654\tAverage Loss: 0.13237255719879149\n",
      "\tTest: Average Accuracy: 0.810438829787234\tAverage Loss: 0.949599911692741\n",
      "Epoch 37:\n",
      "\tTrain: Average Accuracy: 0.966967617270789\tAverage Loss: 0.12813627242349795\n",
      "\tTest: Average Accuracy: 0.8102836879432624\tAverage Loss: 0.9684358767467179\n",
      "Epoch 38:\n",
      "\tTrain: Average Accuracy: 0.9689165778251599\tAverage Loss: 0.12339841286018607\n",
      "\tTest: Average Accuracy: 0.8095744680851064\tAverage Loss: 0.9836855900427793\n",
      "Epoch 39:\n",
      "\tTrain: Average Accuracy: 0.9704157782515992\tAverage Loss: 0.11911145811130505\n",
      "\tTest: Average Accuracy: 0.8069148936170213\tAverage Loss: 1.0019293624024084\n",
      "Epoch 40:\n",
      "\tTrain: Average Accuracy: 0.9713319562899787\tAverage Loss: 0.11512973008370948\n",
      "\tTest: Average Accuracy: 0.8084441489361702\tAverage Loss: 1.0243747675702828\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "trainloader = Dataloader(X_train, y_train, n_classes=20, batch_size=64)\n",
    "testloader = Dataloader(X_test, y_test, n_classes=20, batch_size=64)\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(n_neurons= 60, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 30, initial_weight='uniform', activation=LeakyRelu())\n",
    "network.add_layer(n_neurons= 20, initial_weight='uniform', activation=Identical())\n",
    "network.set_training_param(lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_plt = plt.plot(log['train_accuracy'], label=\"Train\")\n",
    "test_plt = plt.plot(log['test_accuracy'], label=\"Test\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will we always be better with more epochs?\n",
    "No, after some time we reach the state of over fitting in witch the models becomes excelent at identifying train data but will not keep impoving on un seen data witch is test data.\n",
    "as seen in the plot above after about 10 epochs the test accuracy will make very little improvement but the train acc will go upto 97 persent. In this case we say over fitting has occurred and the model hase become less general.<br>\n",
    "### What can we do to improve it?\n",
    "We can detect and stop the model from overfitting witch will decrease time of training and save resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
